{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./tensorboard/PPO_7\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 108      |\n",
      "|    ep_rew_mean     | 1.54e+04 |\n",
      "| time/              |          |\n",
      "|    fps             | 235      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 110          |\n",
      "|    ep_rew_mean          | 1.56e+04     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 221          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.623427e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.147        |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.000245     |\n",
      "|    loss                 | 3.62e+06     |\n",
      "|    n_updates            | 1970         |\n",
      "|    policy_gradient_loss | -7.18e-06    |\n",
      "|    value_loss           | 7.25e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 106           |\n",
      "|    ep_rew_mean          | 1.43e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 230           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 26            |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.2200554e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.145         |\n",
      "|    entropy_loss         | -1.36         |\n",
      "|    explained_variance   | -2.38e-07     |\n",
      "|    learning_rate        | 0.00024       |\n",
      "|    loss                 | 3.25e+06      |\n",
      "|    n_updates            | 1980          |\n",
      "|    policy_gradient_loss | -5.35e-06     |\n",
      "|    value_loss           | 6.51e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 106           |\n",
      "|    ep_rew_mean          | 1.43e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 234           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 34            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8009603e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.142         |\n",
      "|    entropy_loss         | -1.36         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.000235      |\n",
      "|    loss                 | 2.28e+06      |\n",
      "|    n_updates            | 1990          |\n",
      "|    policy_gradient_loss | -3.38e-06     |\n",
      "|    value_loss           | 4.56e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 107           |\n",
      "|    ep_rew_mean          | 1.44e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 234           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 43            |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0110747e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.14          |\n",
      "|    entropy_loss         | -1.35         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00023       |\n",
      "|    loss                 | 2.88e+06      |\n",
      "|    n_updates            | 2000          |\n",
      "|    policy_gradient_loss | -1.46e-06     |\n",
      "|    value_loss           | 5.76e+06      |\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from M2048 import M2048\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "import numpy as np\n",
    "\n",
    "def linear_schedule(initial_value, final_value=0.0):\n",
    "\n",
    "    if isinstance(initial_value, str):\n",
    "        initial_value = float(initial_value)\n",
    "        final_value = float(final_value)\n",
    "        assert (initial_value > 0.0)\n",
    "\n",
    "    def scheduler(progress):\n",
    "        return final_value + progress * (initial_value - final_value)\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "NUM_ENVS = 32\n",
    "make_env = lambda seed=None: Monitor(M2048(4, seed=seed))\n",
    "# env = SubprocVecEnv([make_env(seed=s) for s in np.random.randint(1,1e9, NUM_ENVS)])\n",
    "env = DummyVecEnv([make_env])\n",
    "# env = ActionMasker(env, MinesweeperEnv.get_action_mask)\n",
    "lr_schedule = linear_schedule(2.5e-2, 2.5e-5)\n",
    "clip_range_schedule = linear_schedule(0.15, 0.025)\n",
    "if 1:\n",
    "    model = MaskablePPO(\n",
    "        \"MlpPolicy\", \n",
    "        env=env, \n",
    "        batch_size=2048,\n",
    "        policy_kwargs={\"net_arch\" : [16, 16, 32, 32, 1024, 32, 32, 16, 4]},\n",
    "        verbose=1,\n",
    "        tensorboard_log=\"./tensorboard/\",\n",
    "        learning_rate=lr_schedule,\n",
    "        clip_range=clip_range_schedule,\n",
    "        device='cuda',\n",
    "    )\n",
    "else:\n",
    "    model = MaskablePPO.load(\"./model/2048.pkl\", env=env)\n",
    "model.learn(total_timesteps=1e6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./model/2048.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "range object index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\RL_mineSweeper\\train_2048.ipynb 单元格 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/RL_mineSweeper/train_2048.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m mean_reward, std_reward \u001b[39m=\u001b[39m evaluate_policy(model, env, n_eval_episodes\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RL_mineSweeper/train_2048.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m env\u001b[39m.\u001b[39mclose()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/RL_mineSweeper/train_2048.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mean_reward, std_reward\n",
      "File \u001b[1;32mc:\\Users\\29416\\miniconda3\\envs\\selenium\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:94\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39mwhile\u001b[39;00m (episode_counts \u001b[39m<\u001b[39m episode_count_targets)\u001b[39m.\u001b[39many():\n\u001b[0;32m     88\u001b[0m     actions, states \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(\n\u001b[0;32m     89\u001b[0m         observations,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m     90\u001b[0m         state\u001b[39m=\u001b[39mstates,\n\u001b[0;32m     91\u001b[0m         episode_start\u001b[39m=\u001b[39mepisode_starts,\n\u001b[0;32m     92\u001b[0m         deterministic\u001b[39m=\u001b[39mdeterministic,\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 94\u001b[0m     new_observations, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[0;32m     95\u001b[0m     current_rewards \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rewards\n\u001b[0;32m     96\u001b[0m     current_lengths \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\29416\\miniconda3\\envs\\selenium\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:197\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \n\u001b[0;32m    193\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 197\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\29416\\miniconda3\\envs\\selenium\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     59\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     60\u001b[0m         )\n\u001b[0;32m     61\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\29416\\miniconda3\\envs\\selenium\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "File \u001b[1;32md:\\RL_mineSweeper\\M2048.py:90\u001b[0m, in \u001b[0;36mM2048.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([M2048\u001b[39m.\u001b[39mmerge_tiles(tiles) \u001b[39mfor\u001b[39;00m tiles \u001b[39min\u001b[39;00m rot_map], dtype\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m)\n\u001b[0;32m     89\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrotate_map(action, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 90\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_tile()\n\u001b[0;32m     91\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen_action_mask()\n\u001b[0;32m     93\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_obs(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap\u001b[39m.\u001b[39msum(), \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mFalse\u001b[39;00m, info\n",
      "File \u001b[1;32md:\\RL_mineSweeper\\M2048.py:40\u001b[0m, in \u001b[0;36mM2048.generate_tile\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_tile\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     39\u001b[0m     empty_cells \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m     chioce \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39;49mchoice(\u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(empty_cells[\u001b[39m0\u001b[39;49m])))\n\u001b[0;32m     41\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(empty_cells) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     42\u001b[0m     row, col \u001b[39m=\u001b[39m empty_cells[\u001b[39m0\u001b[39m][chioce], empty_cells[\u001b[39m1\u001b[39m][chioce]\n",
      "File \u001b[1;32mc:\\Users\\29416\\miniconda3\\envs\\selenium\\lib\\random.py:378\u001b[0m, in \u001b[0;36mRandom.choice\u001b[1;34m(self, seq)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Choose a random element from a non-empty sequence.\"\"\"\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[39m# raises IndexError if seq is empty\u001b[39;00m\n\u001b[1;32m--> 378\u001b[0m \u001b[39mreturn\u001b[39;00m seq[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_randbelow(\u001b[39mlen\u001b[39;49m(seq))]\n",
      "\u001b[1;31mIndexError\u001b[0m: range object index out of range"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "env.close()\n",
    "mean_reward, std_reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selenium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
